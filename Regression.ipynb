{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shikha098/python-assignment/blob/main/Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) What is Simple Linear Regression?"
      ],
      "metadata": {
        "id": "yLf9Mtn0pzY6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Linear Regression is a statistical method used to model the relationship between one independent variable (X) and one dependent variable (Y) by fitting a straight line to the observed data. The goal is to predict or explain Y based on X."
      ],
      "metadata": {
        "id": "hNYWXlpCqDJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are the key assumptions of Simple Linear Regression?"
      ],
      "metadata": {
        "id": "laSonG7mqGFT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linearity: The relationship between X and Y is linear.\n",
        "\n",
        "Independence: Observations are independent of each other.\n",
        "\n",
        "Homoscedasticity: The variance of residuals (errors) is constant across all values of X.\n",
        "\n",
        "Normality: Residuals are normally distributed.\n",
        "\n",
        "No multicollinearity: (For simple regression, with one predictor, this isn't relevant.)"
      ],
      "metadata": {
        "id": "g6gRmGXxqLyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) What does the coefficient m represent in the equation Y=mX+c?"
      ],
      "metadata": {
        "id": "GeXWIM-QqW6B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The coefficient m is the slope of the regression line. It represents the expected change in Y for a one-unit increase in X."
      ],
      "metadata": {
        "id": "1_v3My5GqdnX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) What does the intercept c represent in the equation Y=mX+c?"
      ],
      "metadata": {
        "id": "PIerkTCgqwsG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The intercept c is the value of Y when X = 0. It is the point where the regression line crosses the Y-axis."
      ],
      "metadata": {
        "id": "sfryDt-Vq0-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) How do we calculate the slope m in Simple Linear Regression?"
      ],
      "metadata": {
        "id": "mLKR9ALrq56O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The slope\n",
        "ùëö\n",
        "m is calculated using:\n",
        "\n",
        "ùëö\n",
        "=\n",
        "‚àë\n",
        "(\n",
        "ùëã\n",
        "ùëñ\n",
        "‚àí\n",
        "ùëã\n",
        "Àâ\n",
        ")\n",
        "(\n",
        "ùëå\n",
        "ùëñ\n",
        "‚àí\n",
        "ùëå\n",
        "Àâ\n",
        ")\n",
        "‚àë\n",
        "(\n",
        "ùëã\n",
        "ùëñ\n",
        "‚àí\n",
        "ùëã\n",
        "Àâ\n",
        ")\n",
        "2\n",
        "m=\n",
        "‚àë(X\n",
        "i\n",
        "‚Äã\n",
        " ‚àí\n",
        "X\n",
        "Àâ\n",
        " )\n",
        "2\n",
        "\n",
        "‚àë(X\n",
        "i\n",
        "‚Äã\n",
        " ‚àí\n",
        "X\n",
        "Àâ\n",
        " )(Y\n",
        "i\n",
        "‚Äã\n",
        " ‚àí\n",
        "Y\n",
        "Àâ\n",
        " )\n",
        "‚Äã\n",
        "\n",
        "where:\n",
        "\n",
        "ùëã\n",
        "ùëñ\n",
        ",\n",
        "ùëå\n",
        "ùëñ\n",
        "X\n",
        "i\n",
        "‚Äã\n",
        " ,Y\n",
        "i\n",
        "‚Äã\n",
        "  are the individual sample points,\n",
        "\n",
        "ùëã\n",
        "Àâ\n",
        ",\n",
        "ùëå\n",
        "Àâ\n",
        "X\n",
        "Àâ\n",
        " ,\n",
        "Y\n",
        "Àâ\n",
        "  are the means of X and Y respectively."
      ],
      "metadata": {
        "id": "hisOZj8nq--T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6)  What is the purpose of the least squares method in Simple Linear Regression?"
      ],
      "metadata": {
        "id": "mG-vCLEZrEAk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The least squares method minimizes the sum of the squared differences between the observed values (actual Y) and the predicted values (fitted Y). It finds the best-fitting line by reducing the total error."
      ],
      "metadata": {
        "id": "ijFw2yeArKQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7)How is the coefficient of determination (R¬≤) interpreted in Simple Linear Regression?"
      ],
      "metadata": {
        "id": "xX5HGAzErMn3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The\n",
        "ùëÖ\n",
        "2\n",
        "R\n",
        "2\n",
        "  value measures the proportion of variance in the dependent variable (Y) that is explained by the independent variable (X). It ranges from 0 to 1:\n",
        "\n",
        "0: Model explains none of the variability.\n",
        "\n",
        "1: Model explains all the variability.\n",
        "Higher values indicate a better fit."
      ],
      "metadata": {
        "id": "7uja_TDgrRCW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8) What is Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "GtXl3QpgrVhx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple Linear Regression models the relationship between one dependent variable (Y) and two or more independent variables (X1, X2, ..., Xn) by fitting a linear equation that incorporates all predictors."
      ],
      "metadata": {
        "id": "RmA_42cOraYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9) What is the main difference between Simple and Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "MFD4l1-JrhJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Linear Regression: One independent variable.\n",
        "\n",
        "Multiple Linear Regression: Multiple independent variables."
      ],
      "metadata": {
        "id": "LP7oPG7Arnh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10) What are the key assumptions of Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "8lgBPZE0rsZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linearity: Relationship between each independent variable and the dependent variable is linear.\n",
        "\n",
        "Independence: Observations are independent.\n",
        "\n",
        "Homoscedasticity: Constant variance of residuals for all values of predictors.\n",
        "\n",
        "Normality: Residuals are normally distributed.\n",
        "\n",
        "No multicollinearity: Independent variables are not highly correlated with each other."
      ],
      "metadata": {
        "id": "ud32XMCfr7He"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11) What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?"
      ],
      "metadata": {
        "id": "vYedH6Z-sHuJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heteroscedasticity occurs when the variance of the residuals (errors) is not constant across all levels of the independent variables. Instead of having a constant spread, residuals fan out or shrink as predicted values change.\n",
        "\n",
        "Effect:\n",
        "\n",
        "It violates the assumption of homoscedasticity.\n",
        "\n",
        "Causes inefficient estimates and biased standard errors.\n",
        "\n",
        "Makes hypothesis tests (like t-tests) unreliable, potentially leading to incorrect conclusions about significance."
      ],
      "metadata": {
        "id": "Qp_dtxz8s_pV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 12) How can you improve a Multiple Linear Regression model with high multicollinearity?"
      ],
      "metadata": {
        "id": "EZzRLaLXtESU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "High multicollinearity means independent variables are highly correlated, causing instability in coefficient estimates.\n",
        "\n",
        "Ways to improve:\n",
        "\n",
        "Remove or combine correlated predictors.\n",
        "\n",
        "Use Principal Component Analysis (PCA) or Partial Least Squares (PLS) to reduce dimensionality.\n",
        "\n",
        "Use regularization techniques like Ridge or Lasso regression.\n",
        "\n",
        "Collect more data if possible."
      ],
      "metadata": {
        "id": "RoAJLWPdtOQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13) What are some common techniques for transforming categorical variables for use in regression models?"
      ],
      "metadata": {
        "id": "76sov6OXtmks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-hot encoding (dummy variables): Creates binary columns for each category (except one reference category).\n",
        "\n",
        "Label encoding: Assigns numeric codes (less common for regression because it implies order).\n",
        "\n",
        "Ordinal encoding: For categories with a natural order.\n",
        "\n",
        "Effect coding: Similar to dummy variables but compares each category to the overall mean."
      ],
      "metadata": {
        "id": "gQjjamBYtwWg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14) What is the role of interaction terms in Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "A-hThqbZt10n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interaction terms represent the combined effect of two or more variables on the dependent variable that cannot be explained by the individual effects alone.\n",
        "\n",
        "For example, an interaction between X1 and X2 lets the effect of X1 on Y depend on the level of X2."
      ],
      "metadata": {
        "id": "nHzU0_zdt8ff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15) How can the interpretation of the intercept differ between Simple and Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "q4Z4oayrt_fn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Linear Regression: Intercept is the expected value of Y when the single X = 0.\n",
        "\n",
        "Multiple Linear Regression: Intercept is the expected value of Y when all independent variables are zero simultaneously, which might be unrealistic or outside the data range."
      ],
      "metadata": {
        "id": "sViyux6VuExz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16) How can the interpretation of intercept differ between Simple and Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "bnw3NoZKuIei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The slope indicates the expected change in the dependent variable for a one-unit change in the independent variable, holding other variables constant (in multiple regression).\n",
        "\n",
        "It determines the direction and strength of the relationship.\n",
        "\n",
        "Slopes directly affect predictions by scaling the input variables."
      ],
      "metadata": {
        "id": "ng27fcswuNE1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17) How does the intercept in a regression model provide context for the relationship between variables?"
      ],
      "metadata": {
        "id": "3IyXCQu9uR5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The intercept provides a baseline or starting point for the dependent variable when all predictors are zero.\n",
        "\n",
        "It helps anchor the regression line or plane in the outcome space, giving context to how predictors shift the predicted value from this baseline."
      ],
      "metadata": {
        "id": "SH9iOt5UuXM0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18) What are the limitations of using\n",
        "ùëÖ\n",
        "2\n",
        "R\n",
        "2\n",
        "  as a sole measure of model performance"
      ],
      "metadata": {
        "id": "FDBXQ5U6uhar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "R\n",
        "2\n",
        "  always increases when more variables are added, even if they‚Äôre irrelevant.\n",
        "\n",
        "Does not indicate whether the model is biased or if predictions are accurate.\n",
        "\n",
        "Doesn‚Äôt measure whether the regression coefficients are statistically significant.\n",
        "\n",
        "Does not detect overfitting.\n",
        "\n",
        "Can be misleading when comparing models with different numbers of predictors (use adjusted\n",
        "ùëÖ\n",
        "2\n",
        "R\n",
        "2\n",
        "  instead)."
      ],
      "metadata": {
        "id": "nIdXNXu_umB0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 19) How would you interpret a large standard error for a regression coefficient?"
      ],
      "metadata": {
        "id": "x3j6sV1IuqDS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A large standard error means the estimate of that coefficient is less precise.\n",
        "\n",
        "It indicates more variability in the coefficient estimate across samples.\n",
        "\n",
        "Leads to wider confidence intervals and may suggest the coefficient is not statistically significant."
      ],
      "metadata": {
        "id": "RmYsx85cvG-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20) How can heteroscedasticity be identified in residual plots, and why is it important to address it?"
      ],
      "metadata": {
        "id": "W1ehm5zCvKa-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In residual plots, heteroscedasticity appears as a pattern where residual spread varies with fitted values (e.g., funnel shape).\n",
        "\n",
        "Important to address because it violates regression assumptions, leading to incorrect standard errors, confidence intervals, and hypothesis tests.\n",
        "\n",
        "Can be fixed by transforming variables or using heteroscedasticity-robust standard errors."
      ],
      "metadata": {
        "id": "XvPnkW_wvpDz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q21) What does it mean if a Multiple Linear Regression model has a high R¬≤ but low adjusted R¬≤?"
      ],
      "metadata": {
        "id": "OgypiLd_vuIV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This suggests that while the model explains a lot of variance (high\n",
        "ùëÖ\n",
        "2\n",
        "R\n",
        "2\n",
        " ), many predictors may be not meaningful or redundant.\n",
        "\n",
        "Adjusted\n",
        "ùëÖ\n",
        "2\n",
        "R\n",
        "2\n",
        "  penalizes for adding irrelevant predictors, so a low adjusted\n",
        "ùëÖ\n",
        "2\n",
        "R\n",
        "2\n",
        "  indicates potential overfitting or that the extra variables don‚Äôt improve the model enough."
      ],
      "metadata": {
        "id": "NoopzMA_v1Nh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22)Why is it important to scale variables in Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "uiWmaetxv5uC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling is important when:\n",
        "\n",
        "Variables have different units or ranges.\n",
        "\n",
        "You‚Äôre using regularization techniques like Ridge or Lasso (which are sensitive to variable scale).\n",
        "\n",
        "It improves numerical stability and helps optimization algorithms converge faster.\n",
        "\n",
        "It ensures that no variable dominates the model due to its magnitude alone.\n",
        "\n",
        "Common scaling techniques:\n",
        "\n",
        "Standardization (Z-score):\n",
        "(\n",
        "ùë•\n",
        "‚àí\n",
        "ùë•\n",
        "Àâ\n",
        ")\n",
        "/\n",
        "ùúé\n",
        "(x‚àí\n",
        "x\n",
        "Àâ\n",
        " )/œÉ\n",
        "\n",
        "Min-Max Scaling: Scales to a 0‚Äì1 range"
      ],
      "metadata": {
        "id": "LrUlxA8dwO8c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23) What is Polynomial Regression?"
      ],
      "metadata": {
        "id": "3EJww4P_wUoW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomial regression is a type of regression where the relationship between the independent variable and dependent variable is modeled as an nth-degree polynomial.\n",
        "\n",
        "Example (for degree 2):\n",
        "\n",
        "ùëå\n",
        "=\n",
        "ùëè\n",
        "0\n",
        "+\n",
        "ùëè\n",
        "1\n",
        "ùëã\n",
        "+\n",
        "ùëè\n",
        "2\n",
        "ùëã\n",
        "2\n",
        "Y=b\n",
        "0\n",
        "‚Äã\n",
        " +b\n",
        "1\n",
        "‚Äã\n",
        " X+b\n",
        "2\n",
        "‚Äã\n",
        " X\n",
        "2"
      ],
      "metadata": {
        "id": "ZdEnxj_Xwaa9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24) How does Polynomial Regression differ from Linear Regression?\n"
      ],
      "metadata": {
        "id": "5T5HyF5fwdkJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression models a straight-line relationship (only first-degree term:\n",
        "ùëå\n",
        "=\n",
        "ùëè\n",
        "0\n",
        "+\n",
        "ùëè\n",
        "1\n",
        "ùëã\n",
        "Y=b\n",
        "0\n",
        "‚Äã\n",
        " +b\n",
        "1\n",
        "‚Äã\n",
        " X).\n",
        "\n",
        "Polynomial Regression models a curved relationship using powers of the predictor(s), like\n",
        "ùëã\n",
        "2\n",
        ",\n",
        "ùëã\n",
        "3\n",
        "X\n",
        "2\n",
        " ,X\n",
        "3\n",
        " , etc.\n",
        "\n",
        "Technically, polynomial regression is still linear in terms of coefficients, even though the curve is nonlinear in X."
      ],
      "metadata": {
        "id": "bhpAJskIwiLe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25) When is Polynomial Regression used?"
      ],
      "metadata": {
        "id": "WcrDHSvgwmDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When data shows a non-linear relationship between variables.\n",
        "\n",
        "If residual plots from linear regression show curved patterns.\n",
        "\n",
        "When simple linear models underfit the data."
      ],
      "metadata": {
        "id": "8EYqUpl8wrKr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26) What is the general equation for Polynomial Regression?"
      ],
      "metadata": {
        "id": "oyA1sVozwtmE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a single variable and degree n:\n",
        "\n",
        "ùëå\n",
        "=\n",
        "ùëè\n",
        "0\n",
        "+\n",
        "ùëè\n",
        "1\n",
        "ùëã\n",
        "+\n",
        "ùëè\n",
        "2\n",
        "ùëã\n",
        "2\n",
        "+\n",
        "ùëè\n",
        "3\n",
        "ùëã\n",
        "3\n",
        "+\n",
        "‚ãØ\n",
        "+\n",
        "ùëè\n",
        "ùëõ\n",
        "ùëã\n",
        "ùëõ\n",
        "Y=b\n",
        "0\n",
        "‚Äã\n",
        " +b\n",
        "1\n",
        "‚Äã\n",
        " X+b\n",
        "2\n",
        "‚Äã\n",
        " X\n",
        "2\n",
        " +b\n",
        "3\n",
        "‚Äã\n",
        " X\n",
        "3\n",
        " +‚ãØ+b\n",
        "n\n",
        "‚Äã\n",
        " X\n",
        "n\n",
        "\n"
      ],
      "metadata": {
        "id": "FHb6AzjSwzCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27) Can Polynomial Regression be applied to multiple variables?"
      ],
      "metadata": {
        "id": "WxtisZ4aw2Bv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, it can! This is called multivariate polynomial regression.\n",
        "\n",
        "Example (with two variables X and Z, and degree 2):\n",
        "\n",
        "ùëå\n",
        "=\n",
        "ùëè\n",
        "0\n",
        "+\n",
        "ùëè\n",
        "1\n",
        "ùëã\n",
        "+\n",
        "ùëè\n",
        "2\n",
        "ùëç\n",
        "+\n",
        "ùëè\n",
        "3\n",
        "ùëã\n",
        "2\n",
        "+\n",
        "ùëè\n",
        "4\n",
        "ùëç\n",
        "2\n",
        "+\n",
        "ùëè\n",
        "5\n",
        "ùëã\n",
        "ùëç\n",
        "Y=b\n",
        "0\n",
        "‚Äã\n",
        " +b\n",
        "1\n",
        "‚Äã\n",
        " X+b\n",
        "2\n",
        "‚Äã\n",
        " Z+b\n",
        "3\n",
        "‚Äã\n",
        " X\n",
        "2\n",
        " +b\n",
        "4\n",
        "‚Äã\n",
        " Z\n",
        "2\n",
        " +b\n",
        "5\n",
        "‚Äã\n",
        " XZ\n",
        "You include squared terms and interaction terms between variables."
      ],
      "metadata": {
        "id": "pygiegAJw7CI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28) What are the limitations of Polynomial Regression?"
      ],
      "metadata": {
        "id": "Z8RzdJnNw-M4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overfitting: Higher-degree polynomials can fit noise, not just trends.\n",
        "\n",
        "Poor extrapolation: Outside the data range, predictions become unstable.\n",
        "\n",
        "Multicollinearity: Higher-degree terms are often highly correlated.\n",
        "\n",
        "Interpretability: Becomes complex with multiple variables and high degrees."
      ],
      "metadata": {
        "id": "flihAikixCeF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29) What methods can be used to evaluate model fit when selecting the degree of a polynomial?"
      ],
      "metadata": {
        "id": "-0lakwWcxFxZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-validation (k-fold): Helps choose degree that generalizes well.\n",
        "\n",
        "Adjusted R¬≤: Penalizes adding too many terms.\n",
        "\n",
        "AIC/BIC (Akaike/Bayesian Information Criterion): Balances fit with complexity.\n",
        "\n",
        "Plotting residuals: Check if residuals still show a pattern.\n",
        "\n",
        "Learning curves: Evaluate training vs. validation performance."
      ],
      "metadata": {
        "id": "TmKgKKdmxN9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30) Why is visualization important in Polynomial Regression?"
      ],
      "metadata": {
        "id": "UlcuzONzxQkI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helps detect overfitting or underfitting.\n",
        "\n",
        "Visualizes how well the polynomial curve fits the data.\n",
        "\n",
        "Shows the shape of the relationship (e.g., quadratic, cubic).\n",
        "\n",
        "Useful for communicating model behavior, especially in 2D or 3D."
      ],
      "metadata": {
        "id": "6j2aMwX5xVHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "31) How is Polynomial Regression implemented in Python?"
      ],
      "metadata": {
        "id": "6U8L3CSTxYzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Example data\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([2, 6, 14, 28, 45])\n",
        "\n",
        "# Create polynomial regression model of degree 2\n",
        "model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict\n",
        "predictions = model.predict(X)"
      ],
      "metadata": {
        "id": "yd2aNQq0xc_J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}